{
  "affirm": {
    "precision": 0.9,
    "recall": 1.0,
    "f1-score": 0.9473684210526316,
    "support": 27,
    "confused_with": {}
  },
  "send_email": {
    "precision": 1.0,
    "recall": 0.5,
    "f1-score": 0.6666666666666666,
    "support": 12,
    "confused_with": {
      "information": 3,
      "affirm": 3
    }
  },
  "bot_challenge": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 11,
    "confused_with": {}
  },
  "restaurant_search": {
    "precision": 1.0,
    "recall": 0.9838709677419355,
    "f1-score": 0.991869918699187,
    "support": 62,
    "confused_with": {
      "information": 1
    }
  },
  "greet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 17,
    "confused_with": {}
  },
  "deny": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 16,
    "confused_with": {}
  },
  "goodbye": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 30,
    "confused_with": {}
  },
  "information": {
    "precision": 0.9692307692307692,
    "recall": 1.0,
    "f1-score": 0.9843749999999999,
    "support": 126,
    "confused_with": {}
  },
  "micro avg": {
    "precision": 0.9767441860465116,
    "recall": 0.9767441860465116,
    "f1-score": 0.9767441860465116,
    "support": 301
  },
  "macro avg": {
    "precision": 0.9836538461538462,
    "recall": 0.935483870967742,
    "f1-score": 0.9487850008023107,
    "support": 301
  },
  "weighted avg": {
    "precision": 0.9781497572195246,
    "recall": 0.9767441860465116,
    "f1-score": 0.9737745260058825,
    "support": 301
  }
}